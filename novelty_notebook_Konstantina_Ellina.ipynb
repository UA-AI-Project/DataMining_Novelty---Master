{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI PROJECT - NOVELTY\n",
    "\n",
    "Study in the question: When referring to users with variant gaming history, will novelty-optimized recommender systems recommend less popular items than baseline algorithms while maintaining relevance in the recommendations?\n",
    "\n",
    "Konstantina Ellina\n",
    "20230419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the important libraries and define useful functions for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   IMPORT LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import operator\n",
    "from recpack.util import get_top_K_values\n",
    "from recpack.util import get_top_K_ranks\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from metrics.novelty import get_novelty\n",
    "from metrics.coverage import get_coverage\n",
    "from metrics.recall import get_calibrated_recall\n",
    "from metrics.fairness import get_publisher_fairness\n",
    "from metrics.gini_index import get_gini_index\n",
    "from metrics.ils import get_intra_list_similarity\n",
    "from metrics.ndcg import get_ndcg\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   USEFUL FUNCTIONS FOR RECOMMENDER ALGORITHMS BELOW\n",
    "\n",
    "def get_precision_at_k_user(pred: list[int], true: list[int], k: int) -> float: # use the function given in lecture-3 assignment\n",
    "    if k < 0:\n",
    "        raise ValueError(\"k must be greater than 0\")\n",
    "    elif k == 0:\n",
    "        return 1.0\n",
    "\n",
    "    if len(pred) == 0 and len(true) != 0:\n",
    "        return 0.0\n",
    "    elif len(true) == 0:\n",
    "        return 1.0\n",
    "\n",
    "    # convert to set for faster lookup\n",
    "    true_set = set(true)\n",
    "\n",
    "    # get top k predicted items\n",
    "    top_k = pred[:k]\n",
    "\n",
    "    # check how many of these are relevant\n",
    "    correct = 0\n",
    "    for item in top_k:\n",
    "        if item in true_set:\n",
    "            correct += 1\n",
    "            true_set.remove(item)\n",
    "\n",
    "    # return precision at k\n",
    "    return correct / len(top_k)\n",
    "\n",
    "\n",
    "# calculate precision at k using lists\n",
    "def get_precision_at_k(pred: dict[int, list[int]], true: dict[int, list[int]], k): # use the function given in lecture-3 assignment\n",
    "    total_precision_at_k = 0\n",
    "    for user in true.keys():\n",
    "        total_precision_at_k += get_precision_at_k_user(pred[user], true[user], k)\n",
    "    return total_precision_at_k / len(true.keys())\n",
    "\n",
    "\n",
    "def matrix_to_list(mat: csr_matrix, is_sorted: bool = True) -> dict[int, list[int]]: # use the function given in lecture-3 assignment\n",
    "    \"\"\"\n",
    "    A helper function designed to convert a user item matrix into \n",
    "    a dictionary of users and their items. It preserves the order \n",
    "    given by the values in the matrix (0 values are ignored).\n",
    "    :mat: a matrix containing the user item interactions\n",
    "    >>> matrix_to_list(csr_matrix([[1, 0, 1], [0, 1, 0]]))\n",
    "    {0: [0, 2], 1: [1]}\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for user in set(mat.nonzero()[0]):\n",
    "        if is_sorted:\n",
    "            item_rank = [(item, mat[user, item]) for item in mat[user].indices]\n",
    "            sorted_item_rank = sorted(item_rank, key=operator.itemgetter(1))  # sorts the items ascending!\n",
    "            data[user] = list(map(operator.itemgetter(0), sorted_item_rank))\n",
    "        else:\n",
    "            data[user] = [item for item in mat[user].indices]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# calculate item-item similarity\n",
    "def sparse_cosine_similarity(mat: csr_matrix) -> csr_matrix: # use function already made in lecture-3 assignment following the orders there\n",
    "    transp = mat.T\n",
    "    dot_prod = transp.dot(mat) # item-item dot product\n",
    "\n",
    "    l2_norm = np.sqrt(dot_prod.diagonal())\n",
    "    matrix_normalized = np.outer(l2_norm, l2_norm)\n",
    "    matrix_normalized[matrix_normalized == 0] = 1e-9\n",
    "\n",
    "    similarity = dot_prod/matrix_normalized\n",
    "    similarity = similarity.todense()\n",
    "\n",
    "    np.fill_diagonal(similarity, 0) # self similarity will happen so make diagonal 0\n",
    "\n",
    "    return csr_matrix(similarity)\n",
    "\n",
    "\n",
    "# for popularity measure\n",
    "def get_popular_items(interaction_matrix, threshold_percent=90):\n",
    "    item_counts = Counter(interaction_matrix.indices)  # count interactions per item\n",
    "    \n",
    "    # theshold with items in the top 10%\n",
    "    popularity_threshold = np.percentile(list(item_counts.values()), threshold_percent)\n",
    "    \n",
    "    # find the popular items, whose interactions are more than the threshold\n",
    "    popular_items = {item for item, count in item_counts.items() if count >= popularity_threshold}\n",
    "    return popular_items\n",
    "\n",
    "def get_popularity(pred, popular_items, k):\n",
    "    total_recommended = 0\n",
    "    total_popular_recommended = 0\n",
    "\n",
    "    for user, items in pred.items():\n",
    "        top_k_items = items[:k] # choose the top k items in the predictions\n",
    "        total_recommended += len(top_k_items) # count all recommendations\n",
    "        total_popular_recommended += len([item for item in top_k_items if item in popular_items]) # count all popular recommendations\n",
    "    \n",
    "    if total_recommended == 0: # to avoid division with 0\n",
    "        return 0.0\n",
    "    return total_popular_recommended / total_recommended # return the ratio as popularity metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD THE DATASETS NEEDED FOR THE RESEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   LOAD DATASETS\n",
    "\n",
    "games = pd.read_csv('cleaned_datasets_students/games.csv')\n",
    "train_interactions = pd.read_csv('cleaned_datasets_students/train_interactions.csv')\n",
    "test_interactions = pd.read_csv('cleaned_datasets_students/test_interactions_in.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO SOME CLEANING IN THE DATASETS IF NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   check with asserts if further cleaning is needed for dataset(no need here for extra cleaning)\n",
    "\n",
    "def cleaning_datasets(dataset):\n",
    "    ###   CLEANING\n",
    "    dataset = dataset.drop_duplicates() # drop duplicates as cleaning\n",
    "\n",
    "    # check for missing values in all columns\n",
    "    assert dataset.isnull().sum().sum() == 0, \"Dataset has missing values.\"\n",
    "\n",
    "    # identify missing values\n",
    "    missing_values = dataset[dataset.isnull().any(axis=1)]\n",
    "    assert missing_values.empty, f\"These are the rows with the missing values:\\n{missing_values}\"\n",
    "\n",
    "    # check for invalid IDs\n",
    "    invalid_ids = dataset[(dataset['user_id'] < 0) | (dataset['item_id'] < 0)]\n",
    "    assert invalid_ids.empty, f\"These are the rows with the invalid IDs:\\n{invalid_ids}\"\n",
    "\n",
    "    # check for missing or incorrect playtime\n",
    "    missing_playtime = dataset[dataset['playtime'].isnull()]\n",
    "    assert missing_playtime.empty, f\"These are the rows with the missing playtime:\\n{missing_playtime}\"\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_interactions = cleaning_datasets(train_interactions)\n",
    "test_interactions = cleaning_datasets(test_interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra items in test set: {np.int64(7168), np.int64(4103), np.int64(4106), np.int64(8202), np.int64(6166), np.int64(2590), np.int64(6183), np.int64(7735), np.int64(6204), np.int64(6716), np.int64(5703), np.int64(7250), np.int64(3667), np.int64(6228), np.int64(599), np.int64(4695), np.int64(6745), np.int64(6747), np.int64(4197), np.int64(7269), np.int64(7274), np.int64(6251), np.int64(7285), np.int64(3705), np.int64(5756), np.int64(7292), np.int64(2689), np.int64(3206), np.int64(4743), np.int64(3730), np.int64(4242), np.int64(5782), np.int64(154), np.int64(7322), np.int64(2716), np.int64(4253), np.int64(7841), np.int64(4775), np.int64(6312), np.int64(4286), np.int64(5313), np.int64(5825), np.int64(5315), np.int64(4298), np.int64(719), np.int64(6868), np.int64(3286), np.int64(3287), np.int64(4822), np.int64(4825), np.int64(5850), np.int64(6876), np.int64(3805), np.int64(4833), np.int64(4323), np.int64(6373), np.int64(5869), np.int64(3822), np.int64(4846), np.int64(6385), np.int64(7921), np.int64(8191), np.int64(7932), np.int64(7427), np.int64(4358), np.int64(4365), np.int64(7955), np.int64(2845), np.int64(4386), np.int64(7972), np.int64(5418), np.int64(1851), np.int64(4923), np.int64(2878), np.int64(4419), np.int64(3912), np.int64(3401), np.int64(7497), np.int64(6480), np.int64(5457), np.int64(4950), np.int64(3417), np.int64(3418), np.int64(7005), np.int64(5983), np.int64(8047), np.int64(3441), np.int64(5497), np.int64(7034), np.int64(6013), np.int64(7549), np.int64(4479), np.int64(3970), np.int64(4995), np.int64(6018), np.int64(6020), np.int64(7044), np.int64(3975), np.int64(3976), np.int64(5511), np.int64(5514), np.int64(7562), np.int64(5516), np.int64(2967), np.int64(8090), np.int64(8116), np.int64(2997), np.int64(7607), np.int64(4539), np.int64(5573), np.int64(7109), np.int64(7621), np.int64(1992), np.int64(2507), np.int64(6109), np.int64(8159), np.int64(8161), np.int64(6116), np.int64(8172), np.int64(6637), np.int64(8178), np.int64(4598), np.int64(8190), np.int64(4095)}\n",
      "The extra items are: 124\n",
      "\n",
      "Extra items were excluded from original test dataset.\n",
      "Original test interactions: 448211\n",
      "Filtered test interactions: 448075\n"
     ]
    }
   ],
   "source": [
    "###   CHECK IF ALL ITEMS IN TEST DATASET ARE ALSO IN TRAIN DATASET, AND IF NOT THEN EXCLUDE THEM. useful for evaluation later\n",
    "\n",
    "def check_missing_items(train_interactions, test_interactions):\n",
    "    train_items = set(train_interactions['item_id'].unique())\n",
    "    test_items = set(test_interactions['item_id'].unique())\n",
    "    missing_items = test_items - train_items # find the missing items\n",
    "\n",
    "    if missing_items:\n",
    "        print(f\"Extra items in test set: {missing_items}\")\n",
    "    else:\n",
    "        print(\"Train and test have the same items.\")\n",
    "\n",
    "    return missing_items\n",
    "\n",
    "missing_items = check_missing_items(train_interactions, test_interactions)\n",
    "\n",
    "if len(missing_items) != 0: # if there are missing items\n",
    "    print('The extra items are:', len(missing_items))\n",
    "    filtered_test_interactions = test_interactions[~test_interactions['item_id'].isin(missing_items)] # exclude them\n",
    "    print(\"\\nExtra items were excluded from original test dataset.\")\n",
    "    print(f\"Original test interactions: {len(test_interactions)}\")\n",
    "    print(f\"Filtered test interactions: {len(filtered_test_interactions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPARSE MATRIX - After cleaning, create sparse matrix to work with in the rest of the research (followed the method given in the assignment in the course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE SPARSE MATRICES FOR TRAIN AND TEST INTERACTIONS\n",
    "\n",
    "# FOR TRAIN INTERACTIONS\n",
    "train_user_id_mapping = {user_id: idx for idx, user_id in enumerate(train_interactions['user_id'].unique())} # user id mapping\n",
    "train_interactions['new_user_id'] = train_interactions['user_id'].map(train_user_id_mapping) # apply on train\n",
    "\n",
    "item_id_mapping = {item_id: idx for idx, item_id in enumerate(train_interactions['item_id'].unique())} # item id mapping(this will be the same in test datasets bc of same items)\n",
    "train_interactions['new_item_id'] = train_interactions['item_id'].map(item_id_mapping) # apply on train\n",
    "\n",
    "# keep mappings as dictionaries for later\n",
    "train_new_to_old_user_id_mapping = {v: k for k, v in train_user_id_mapping.items()}\n",
    "new_to_old_item_id_mapping = {v: k for k, v in item_id_mapping.items()}\n",
    "\n",
    "# attributes for train matrix and apply them to csr_matrix to create one\n",
    "num_train_users = train_interactions['new_user_id'].nunique()\n",
    "num_items = train_interactions['new_item_id'].nunique()\n",
    "train_rows = train_interactions['new_user_id'].values\n",
    "train_cols = train_interactions['new_item_id'].values\n",
    "train_data = train_interactions['playtime'].values  # use playtime as data for sparse matrix\n",
    "\n",
    "# create the matrix using the attributes above\n",
    "train_interaction_matrix_csr = csr_matrix((train_data, (train_rows, train_cols)), shape=(num_train_users, num_items))\n",
    "\n",
    "\n",
    "# FOR TEST INTERACTIONS (use filtered_test_interactions here)\n",
    "filtered_test_interactions = filtered_test_interactions.copy() # create copy bc of warnings\n",
    "test_user_id_mapping = {user_id: idx for idx, user_id in enumerate(filtered_test_interactions['user_id'].unique())} # different user id mapping\n",
    "filtered_test_interactions['new_user_id'] = filtered_test_interactions['user_id'].map(test_user_id_mapping) # apply on test\n",
    "filtered_test_interactions['new_item_id'] = filtered_test_interactions['item_id'].map(item_id_mapping) # apply same item id mapping to test\n",
    "\n",
    "test_new_to_old_user_id_mapping = {v: k for k, v in test_user_id_mapping.items()} # keep the user id mapping\n",
    "\n",
    "# attributes for test matrix and apply them to csr_matrix to create one\n",
    "num_test_users = filtered_test_interactions['new_user_id'].nunique()\n",
    "test_rows = filtered_test_interactions['new_user_id'].values\n",
    "test_cols = filtered_test_interactions['new_item_id'].values\n",
    "test_data = filtered_test_interactions['playtime'].values\n",
    "\n",
    "# create the matrix using the attributes above\n",
    "test_interaction_matrix_csr = csr_matrix((test_data, (test_rows, test_cols)), shape=(num_test_users, num_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTERING - In this research, we are interested in the users that have played more than 3 games and more than 200 minutes in general. So here I filter the users and change the id mapping as needed(followed the methods shown in assignments in the course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   FILTER OUT USERS WITH SMALL GAMING HISTORY FROM SPARSE MATRIX\n",
    "\n",
    "def MinItemsPerUser(interaction_matrix_csr, user_mapping, min_items, min_time):\n",
    "    user_interactions_mask = interaction_matrix_csr.getnnz(axis=1) >= min_items # filter out users with strictly less than 3 interactions\n",
    "    user_time_mask = interaction_matrix_csr.sum(axis=1).A1 >= min_time # filter out users with strictly less than 200 mins of playtime\n",
    "    user_total_mask = user_interactions_mask & user_time_mask # create the total masks with both filters\n",
    "    \n",
    "    filtered_interaction_matrix = interaction_matrix_csr[user_total_mask] # matrix with filters\n",
    "\n",
    "    # update the ids again\n",
    "    updated_user_mapping = {new_idx: user_mapping[old_idx] for new_idx, old_idx in enumerate(np.where(user_total_mask)[0])}\n",
    "    \n",
    "    return filtered_interaction_matrix, updated_user_mapping\n",
    "\n",
    "# apply the filters and get the new filtered matrix and the new id mapping both for train and test but separately\n",
    "train_interaction_matrix_csr, train_new_to_old_user_id_mapping = MinItemsPerUser(train_interaction_matrix_csr, train_new_to_old_user_id_mapping, min_items=3, min_time=200)\n",
    "test_interaction_matrix_csr, test_new_to_old_user_id_mapping = MinItemsPerUser(test_interaction_matrix_csr, test_new_to_old_user_id_mapping, min_items=3, min_time=200)\n",
    "\n",
    "# check if the filters are applied correctly and all users have more than 3 interactions and more than 600 mins playtime\n",
    "assert train_interaction_matrix_csr.getnnz(axis=1).min() >= 3\n",
    "assert train_interaction_matrix_csr.sum(axis=1).min() >= 200\n",
    "assert test_interaction_matrix_csr.getnnz(axis=1).min() >= 3\n",
    "assert test_interaction_matrix_csr.sum(axis=1).min() >= 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST SPLIT - split the sparse matrix of test_interactions to use half of it for the predicted recommendations and half of it for the true recommendations used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   SPLIT TEST_INTERACTIONS_MATRIX_CSR\n",
    "\n",
    "np.random.seed(42) # define for reproducability\n",
    "\n",
    "# get the data needed from the dataset\n",
    "nonzero_indices = test_interaction_matrix_csr.nonzero()\n",
    "nonzero_data = test_interaction_matrix_csr.data\n",
    "num_test_interactions = len(nonzero_data)\n",
    "\n",
    "shuffled_indices = np.random.permutation(num_test_interactions) # suffle the indices for randomness\n",
    "split_point = num_test_interactions // 2 # split in half\n",
    "\n",
    "# define the indices depending on the split point\n",
    "test_mat_indices = shuffled_indices[:split_point]\n",
    "test_true_indices = shuffled_indices[split_point:]\n",
    "\n",
    "# create test_mat as the matrices before (this is used for the predictions)\n",
    "test_mat_rows = nonzero_indices[0][test_mat_indices]\n",
    "test_mat_cols = nonzero_indices[1][test_mat_indices]\n",
    "test_mat_data = nonzero_data[test_mat_indices]\n",
    "test_mat = csr_matrix((test_mat_data, (test_mat_rows, test_mat_cols)), shape=test_interaction_matrix_csr.shape) # make a new sparse matrix for the predictions\n",
    "\n",
    "# create test_true as the matrices before (this is used for the true recommendations)\n",
    "test_true_rows = nonzero_indices[0][test_true_indices]\n",
    "test_true_cols = nonzero_indices[1][test_true_indices]\n",
    "test_true_data = nonzero_data[test_true_indices]\n",
    "test_true = csr_matrix((test_true_data, (test_true_rows, test_true_cols)), shape=test_interaction_matrix_csr.shape) # make a new sparse matrix for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMILARITY AND TRUE RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   FIND TOP-K MOST SIMILAR ITEMS AND CALCULATE TRUE RECOMMENDATIONS\n",
    "\n",
    "# item-item similarity to see how similar one item is to another based on user interactions\n",
    "item_item_similarity = sparse_cosine_similarity(train_interaction_matrix_csr) # use train dataset here\n",
    "\n",
    "# get top-n most similar items for each item in the similarity matrix\n",
    "top_n = 20\n",
    "item_item_similarity = get_top_K_values(item_item_similarity, top_n)\n",
    "\n",
    "# this is the same true recommendations for every baseline algorithm and it is going to be used for comparison reasons\n",
    "true_recommendations = matrix_to_list(test_true) # use test_true dataset here so we can evaluate models on train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASELINE AND NOVELTY-OPTIMIZED ALGORITHMS - Define the baseline algorithms and the novelty-optimized algorithms and find recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   ITEM-KNN BASELINE ALGORITHM\n",
    "\n",
    "def item_knn_recommend(sim: csr_matrix, test: csr_matrix, n: int) -> dict[int, list[int]]: # recommend function following again the orders in lecture-3 assignment\n",
    "    scores_matrix = test.dot(sim.T)\n",
    "    scores_matrix = np.array(scores_matrix.todense()) # scores indicating the relevance of an item to a user\n",
    "\n",
    "    recommendations = {}\n",
    "    users = test.shape[0]\n",
    "    for user in range(users): # for each user generate their top recommendations\n",
    "        scores = scores_matrix[user]\n",
    "\n",
    "        scores_sparse = csr_matrix(scores)\n",
    "        top_n_ranks = get_top_K_ranks(scores_sparse, n) # top n items based on the scores\n",
    "\n",
    "        highest_items = top_n_ranks.indices[top_n_ranks.data.argsort()]\n",
    "\n",
    "        recommendations[user] = highest_items.tolist() # save top n recommended items for the user\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# calculate predicted recommendations\n",
    "n = 20 # number of recommendations per user\n",
    "pred_item_knn_baseline = item_knn_recommend(item_item_similarity, test_mat, n) # use test_mat in all the algorithms to find the recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   USER-KNN BASELINE ALGORITHM\n",
    "\n",
    "def user_knn_recommend(train: csr_matrix, test: csr_matrix, k:int, n:int) -> dict[int, list[int]]:\n",
    "    nn = NearestNeighbors(n_neighbors=k+1, metric='cosine', algorithm='auto', n_jobs=-1) # initialize the algorithm\n",
    "    nn.fit(train) # fit to train matrix to compute the similar users\n",
    "    \n",
    "    # for each user in the test set find top-k similar users in train matrix\n",
    "    distances, indices = nn.kneighbors(test)\n",
    "\n",
    "    recommendations = {}\n",
    "    for user, test_user_similar_users in enumerate(indices):\n",
    "        similar_users = test_user_similar_users[1:]  # exclude the user itself\n",
    "        \n",
    "        similar_users_interactions = train[similar_users].sum(axis=0).A1  # sum interactions across items from similar users in train\n",
    "        \n",
    "        # top n items\n",
    "        recommended_items = np.argpartition(-similar_users_interactions, range(n))[:n]\n",
    "        recommendations[user] = recommended_items.tolist() # and save them to the dictionary\n",
    "        \n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# calculate predicted recommendations\n",
    "n = 20\n",
    "k = 10 # number of similar users\n",
    "pred_user_knn_baseline = user_knn_recommend(train_interaction_matrix_csr, test_mat, k, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   NOVELTY-ONLY ITEM-KNN ALGORITHM\n",
    "\n",
    "# this function recommends only games the users haven't played\n",
    "def item_knn_only_novel_recommend(sim: csr_matrix, test: csr_matrix, n: int) -> dict[int, list[int]]:\n",
    "    scores_matrix = test.dot(sim.T)\n",
    "    scores_matrix = np.array(scores_matrix.todense()) # scores indicating the relevance of an item to a user\n",
    "\n",
    "    recommendations = {}\n",
    "    users = test.shape[0]\n",
    "    for user in range(users): # for each user generate their top recommendations\n",
    "        scores = scores_matrix[user]\n",
    "\n",
    "        interacted_items = test[user].indices # keep games already played by the user\n",
    "        scores[interacted_items] = -np.inf # exclude them from scoring with giving them -infinity score(lowest)\n",
    "\n",
    "        scores_sparse = csr_matrix(scores)\n",
    "        top_n_ranks = get_top_K_ranks(scores_sparse, n) # top n items based on the scores\n",
    "\n",
    "        highest_items = top_n_ranks.indices[top_n_ranks.data.argsort()]\n",
    "\n",
    "        recommendations[user] = highest_items.tolist() # save top n recommended items for the user\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# calculate predicted recommendations\n",
    "n = 20\n",
    "pred_item_knn_only_novel = item_knn_only_novel_recommend(item_item_similarity, test_mat, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   NOVELTY-ONLY USER-KNN ALGORITHM\n",
    "\n",
    "def user_knn_only_novel_recommend(train: csr_matrix, test: csr_matrix, k:int, n:int) -> dict[int, list[int]]: # same as user-knn but now recommend only novel items\n",
    "    nn = NearestNeighbors(n_neighbors=k+1, metric='cosine', algorithm='auto', n_jobs=-1)\n",
    "    nn.fit(train)\n",
    "    \n",
    "    distances, indices = nn.kneighbors(test)\n",
    "    \n",
    "    recommendations = {}\n",
    "    for user, test_user_similar_users in enumerate(indices):\n",
    "        similar_users = test_user_similar_users[1:]  # exclude the user\n",
    "        \n",
    "        similar_user_items = train[similar_users].sum(axis=0).A1  # sum interactions across items from similar users in train\n",
    "        \n",
    "        # exclude items the user has already interacted with in the test matrix\n",
    "        user_interactions = test[user].indices\n",
    "        similar_user_items[user_interactions] = -999999\n",
    "\n",
    "        # top-n items for recommendation\n",
    "        recommended_items = np.argpartition(-similar_user_items, range(n))[:n]\n",
    "        recommendations[user] = recommended_items.tolist()\n",
    "        \n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# calculate predicted recommendations\n",
    "n = 20\n",
    "k = 10\n",
    "pred_user_knn_only_novel = user_knn_only_novel_recommend(train_interaction_matrix_csr, test_mat, k, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   HYBRID NOVELTY RECOMMENDER\n",
    "\n",
    "# recommend mostly novel items but still allow highly relevant previously seen items if they have a big score\n",
    "def hybrid_novelty_recommend(sim: csr_matrix, test: csr_matrix, n: int, novelty_weight) -> dict[int, list[int]]: # uses item-knn method\n",
    "    scores_matrix = test.dot(sim.T).toarray()\n",
    "\n",
    "    recommendations = {}\n",
    "    users = test.shape[0]\n",
    "    for user in range(users):\n",
    "        scores = scores_matrix[user]\n",
    "        \n",
    "        interacted_items = test[user].indices\n",
    "        scores[interacted_items] *= (1 - novelty_weight)  # low the score for already seen items(if still bigger than novel items then recommend it)\n",
    "\n",
    "        scores_sparse = csr_matrix(scores)\n",
    "        top_n_ranks = get_top_K_ranks(scores_sparse, n) # top n items based on the scores\n",
    "\n",
    "        highest_items = top_n_ranks.indices[top_n_ranks.data.argsort()]\n",
    "\n",
    "        recommendations[user] = highest_items.tolist() # save top n recommended items for the user\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "\n",
    "# calculate predicted recommendations\n",
    "n = 20\n",
    "pred_hybrid_novelty_03 = hybrid_novelty_recommend(item_item_similarity, test_mat, n, novelty_weight=0.3) # more relevance\n",
    "pred_hybrid_novelty_05 = hybrid_novelty_recommend(item_item_similarity, test_mat, n, novelty_weight=0.5) # balanced\n",
    "pred_hybrid_novelty_08 = hybrid_novelty_recommend(item_item_similarity, test_mat, n, novelty_weight=0.8) # more novelty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SANITY CHECKS - do some checks in the predictions to see if everything flows correctly\n",
    "\n",
    "- check the stability of the recommender algorithm to see if they give the same results every time\n",
    "- check for duplicate predictions in the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-KNN baseline : predictions are stable across iterations.\n",
      "User-KNN baseline : predictions are stable across iterations.\n",
      "Novelty-only Item-KNN : predictions are stable across iterations.\n",
      "Novelty-only User-KNN : predictions are stable across iterations.\n",
      "Hybrid algorithm with more relevance : predictions are stable across iterations.\n",
      "Hybrid algorithm with balanced relevance and novelty : predictions are stable across iterations.\n",
      "Hybrid algorithm with more novelty : predictions are stable across iterations.\n"
     ]
    }
   ],
   "source": [
    "# run each algorithm 3 times to see if the results stay the same each time\n",
    "\n",
    "def check_stability(stability_results, name): # check for the stability itself here\n",
    "    for user in stability_results[0].keys():\n",
    "        all_predictions = [set(pred[user]) for pred in stability_results]\n",
    "        if not all(pred == all_predictions[0] for pred in all_predictions):\n",
    "            print(f\"Warning: Predictions for user {user} are not stable across iterations.\")\n",
    "            return\n",
    "    print(name, \": predictions are stable across iterations.\")\n",
    "\n",
    "# run for 3 times all the algorithms\n",
    "def run_predictions(model, item_item_similarity, train_matrix, test_fold_in, name, iterations=3, n=20, k=10):\n",
    "    stability_results = []\n",
    "    for _ in range(iterations):\n",
    "        if name == \"Item-KNN baseline\":\n",
    "            predictions = model(item_item_similarity, test_fold_in, n)\n",
    "        if name == \"User-KNN baseline\":\n",
    "            predictions = model(train_matrix, test_fold_in, k, n)\n",
    "        if name == \"Novelty-only Item-KNN\":\n",
    "            predictions = model(item_item_similarity, test_fold_in, n)\n",
    "        if name == \"Novelty-only User-KNN\":\n",
    "            predictions = model(train_matrix, test_fold_in, k, n)\n",
    "        if name == \"Hybrid algorithm with more relevance\":\n",
    "            predictions = model(item_item_similarity, test_fold_in, n, novelty_weight=0.3)\n",
    "        if name == \"Hybrid algorithm with balanced relevance and novelty\":\n",
    "            predictions = model(item_item_similarity, test_fold_in, n, novelty_weight=0.5)\n",
    "        if name == \"Hybrid algorithm with more novelty\":\n",
    "            predictions = model(item_item_similarity, test_fold_in, n, novelty_weight=0.7)\n",
    "\n",
    "        stability_results.append(predictions)\n",
    "    check_stability(stability_results=stability_results, name=name)\n",
    "\n",
    "\n",
    "run_predictions(item_knn_recommend, item_item_similarity, train_interaction_matrix_csr, test_mat, name=\"Item-KNN baseline\")\n",
    "run_predictions(user_knn_recommend, item_item_similarity, train_interaction_matrix_csr, test_mat, name=\"User-KNN baseline\")\n",
    "run_predictions(item_knn_only_novel_recommend, item_item_similarity, train_interaction_matrix_csr, test_mat, name=\"Novelty-only Item-KNN\")\n",
    "run_predictions(user_knn_only_novel_recommend, item_item_similarity, train_interaction_matrix_csr, test_mat, name=\"Novelty-only User-KNN\")\n",
    "run_predictions(hybrid_novelty_recommend, item_item_similarity, train_interaction_matrix_csr, test_mat, name=\"Hybrid algorithm with more relevance\")\n",
    "run_predictions(hybrid_novelty_recommend, item_item_similarity, train_interaction_matrix_csr, test_mat, name=\"Hybrid algorithm with balanced relevance and novelty\")\n",
    "run_predictions(hybrid_novelty_recommend, item_item_similarity, train_interaction_matrix_csr, test_mat, name=\"Hybrid algorithm with more novelty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate recommendations found.\n",
      "No duplicate recommendations found.\n",
      "No duplicate recommendations found.\n",
      "No duplicate recommendations found.\n",
      "No duplicate recommendations found.\n",
      "No duplicate recommendations found.\n",
      "No duplicate recommendations found.\n"
     ]
    }
   ],
   "source": [
    "# in the predicted items, check if there are duplicates\n",
    "def check_duplicate_predictions(pred_content):\n",
    "    for user, items in pred_content.items():\n",
    "        if len(items) != len(set(items)):\n",
    "            print(f\"Warning: User {user} has duplicate recommendations: {items}\")\n",
    "            return\n",
    "    print(\"No duplicate recommendations found.\")\n",
    "\n",
    "check_duplicate_predictions(pred_item_knn_baseline)\n",
    "check_duplicate_predictions(pred_user_knn_baseline)\n",
    "check_duplicate_predictions(pred_item_knn_only_novel)\n",
    "check_duplicate_predictions(pred_user_knn_only_novel)\n",
    "check_duplicate_predictions(pred_hybrid_novelty_03)\n",
    "check_duplicate_predictions(pred_hybrid_novelty_05)\n",
    "check_duplicate_predictions(pred_hybrid_novelty_08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION - for each algorithm, run the metrics given(plus the popularity measure) and get results for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item-KNN baseline\n",
      "NDCG: 0.1537\n",
      "Recall: 0.1681\n",
      "Coverage: 0.5192\n",
      "Intra list: 0.0851\n",
      "Gini index: 0.8825\n",
      "Publisher fairness: 0.9047\n",
      "Novelty: 0.9352\n",
      "Popularity: 0.7927\n",
      "\n",
      "User-KNN baseline\n",
      "NDCG: 0.1250\n",
      "Recall: 0.1757\n",
      "Coverage: 0.2408\n",
      "Intra list: 0.0678\n",
      "Gini index: 0.8762\n",
      "Publisher fairness: 0.8931\n",
      "Novelty: 0.9460\n",
      "Popularity: 0.9497\n",
      "\n",
      "Novelty-only Item-KNN\n",
      "NDCG: 0.1680\n",
      "Recall: 0.1797\n",
      "Coverage: 0.5256\n",
      "Intra list: 0.0788\n",
      "Gini index: 0.8785\n",
      "Publisher fairness: 0.9012\n",
      "Novelty: 0.9370\n",
      "Popularity: 0.7806\n",
      "\n",
      "Novelty-only User KNN\n",
      "NDCG: 0.1824\n",
      "Recall: 0.1989\n",
      "Coverage: 0.2695\n",
      "Intra list: 0.0574\n",
      "Gini index: 0.8655\n",
      "Publisher fairness: 0.8869\n",
      "Novelty: 0.9490\n",
      "Popularity: 0.9395\n",
      "\n",
      "Hybrid algorithm with more relevance \n",
      "NDCG: 0.1627\n",
      "Recall: 0.1738\n",
      "Coverage: 0.5210\n",
      "Intra list: 0.0830\n",
      "Gini index: 0.8811\n",
      "Publisher fairness: 0.9033\n",
      "Novelty: 0.9357\n",
      "Popularity: 0.7882\n",
      "\n",
      "Hybrid algorithm with balanced relevance and novelty\n",
      "NDCG: 0.1663\n",
      "Recall: 0.1776\n",
      "Coverage: 0.5223\n",
      "Intra list: 0.0804\n",
      "Gini index: 0.8799\n",
      "Publisher fairness: 0.9025\n",
      "Novelty: 0.9364\n",
      "Popularity: 0.7847\n",
      "\n",
      "Hybrid algorithm with more novelty\n",
      "NDCG: 0.1679\n",
      "Recall: 0.1796\n",
      "Coverage: 0.5227\n",
      "Intra list: 0.0788\n",
      "Gini index: 0.8785\n",
      "Publisher fairness: 0.9012\n",
      "Novelty: 0.9369\n",
      "Popularity: 0.7812\n"
     ]
    }
   ],
   "source": [
    "###   EVALUATION\n",
    "\n",
    "popular_items = get_popular_items(train_interaction_matrix_csr, threshold_percent=90)\n",
    "predictions = [pred_item_knn_baseline, pred_user_knn_baseline, pred_item_knn_only_novel, pred_user_knn_only_novel, pred_hybrid_novelty_03, pred_hybrid_novelty_05, pred_hybrid_novelty_08]\n",
    "methods = ['Item-KNN baseline','User-KNN baseline','Novelty-only Item-KNN','Novelty-only User KNN','Hybrid algorithm with more relevance ','Hybrid algorithm with balanced relevance and novelty', 'Hybrid algorithm with more novelty']\n",
    "n_items = train_interaction_matrix_csr.shape[1]  # total items in train dataset(for coverage)\n",
    "item_publisher = games.set_index('item_id')['publisher'].to_dict()\n",
    "i2p_dict = {i: item_publisher.get(i, \"Unknown\") for i in range(max(item_publisher.keys()) + 1)}\n",
    "n=20\n",
    "\n",
    "# function to get all the metrics results\n",
    "def evaluation_metrics(predictions, method_names, train, test, n_items, true, i2p_dict, popular_items, n):\n",
    "    for method_name, pred in zip(method_names, predictions):\n",
    "        print(f'\\n{method_name}')\n",
    "        print(f'NDCG: {get_ndcg(pred, true, n):.4f}')\n",
    "        print(f'Recall: {get_calibrated_recall(pred, true, n):.4f}')\n",
    "        print(f'Coverage: {get_coverage(pred, n_items, n):.4f}')\n",
    "        print(f'Intra list: {get_intra_list_similarity(pred, train, n):.4f}')\n",
    "        print(f'Gini index: {get_gini_index(pred, n):.4f}')\n",
    "        print(f'Publisher fairness: {get_publisher_fairness(pred, i2p_dict, n):.4f}')\n",
    "        print(f'Novelty: {get_novelty(pred, train, test, n):.4f}')\n",
    "        print(f'Popularity: {get_popularity(pred, popular_items, n):.4f}')\n",
    "\n",
    "\n",
    "evaluation_metrics(predictions=predictions, method_names=methods, train=train_interaction_matrix_csr, test=test_mat, n_items=n_items, true=true_recommendations, i2p_dict=i2p_dict, popular_items=popular_items, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE RESULTS - save all predictions on csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pred_item_knn_baseline.csv\n",
      "Saved pred_user_knn_baseline.csv\n",
      "Saved pred_item_knn_only_novel.csv\n",
      "Saved pred_user_knn_only_novel.csv\n",
      "Saved pred_hybrid_novelty_03.csv\n",
      "Saved pred_hybrid_novelty_05.csv\n",
      "Saved pred_hybrid_novelty_08.csv\n"
     ]
    }
   ],
   "source": [
    "###   SAVE RESULTS\n",
    "\n",
    "predictions = [\n",
    "    \"pred_item_knn_baseline\", \"pred_user_knn_baseline\", \n",
    "    \"pred_item_knn_only_novel\", \"pred_user_knn_only_novel\", \n",
    "    \"pred_hybrid_novelty_03\", \"pred_hybrid_novelty_05\", \n",
    "    \"pred_hybrid_novelty_08\"\n",
    "]\n",
    "predictions_dict = {\n",
    "    \"pred_item_knn_baseline\": pred_item_knn_baseline,\n",
    "    \"pred_user_knn_baseline\": pred_user_knn_baseline,\n",
    "    \"pred_item_knn_only_novel\": pred_item_knn_only_novel,\n",
    "    \"pred_user_knn_only_novel\": pred_user_knn_only_novel,\n",
    "    \"pred_hybrid_novelty_03\": pred_hybrid_novelty_03,\n",
    "    \"pred_hybrid_novelty_05\": pred_hybrid_novelty_05,\n",
    "    \"pred_hybrid_novelty_08\": pred_hybrid_novelty_08\n",
    "}\n",
    "\n",
    "for pred in predictions:\n",
    "    prediction_content = predictions_dict[pred]\n",
    "\n",
    "    # map back to original ids\n",
    "    data = []\n",
    "    for user_id, item_ids in prediction_content.items():\n",
    "        original_user_id = test_new_to_old_user_id_mapping[user_id]  # map to original user id\n",
    "        original_item_ids = [new_to_old_item_id_mapping[item_id] for item_id in item_ids]  # map to original item ids\n",
    "        data.extend([(original_user_id, original_item_id) for original_item_id in original_item_ids])\n",
    "\n",
    "    # create new dataframe with correct ids\n",
    "    df = pd.DataFrame(data, columns=[\"user_id\", \"item_id\"])\n",
    "    \n",
    "    # save this new dataframe as a csv to use for codabench\n",
    "    csv_path = f\"{pred}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
